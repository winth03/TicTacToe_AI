{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import random\n",
    "import ttt as env\n",
    "\n",
    "STATES = 19683\n",
    "ACTIONS = 9\n",
    "\n",
    "Q1 = defaultdict(lambda:np.array([0.6]*ACTIONS))\n",
    "Q2 = deepcopy(Q1)\n",
    "\n",
    "EPISODES = 5000 # how many times to run the enviornment from the beginning\n",
    "MAX_STEPS = 20  # max number of steps allowed for each run of enviornment\n",
    "\n",
    "LEARNING_RATE = 0.81  # learning rate\n",
    "GAMMA = 0.96\n",
    "\n",
    "RENDER = False # if you want to see training set to true\n",
    "\n",
    "epsilon = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ttt' from 'd:\\\\Projects\\\\TicTacToe_AI\\\\ttt.py'>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 / 5000 100.00%\n",
      "Average reward for X: 0.8854770954190838: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Average reward for O: -0.8789757951590318: [-1.0, -1.0, -1.0, -1.0, -1.0]\n"
     ]
    }
   ],
   "source": [
    "rewards1 = []\n",
    "rewards2 = []\n",
    "for episode in range(EPISODES):\n",
    "  if (episode+1) % 1000 == 0:\n",
    "    clear_output()\n",
    "    print(f\"{(episode+1)} / {EPISODES} {((episode+1)/EPISODES)*100:.2f}%\", flush=True)\n",
    "\n",
    "  state = env.reset_game()\n",
    "  for _ in range(MAX_STEPS):\n",
    "\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "      action = env.get_action()\n",
    "    else:\n",
    "      action = np.argmax(Q1[state])\n",
    "\n",
    "    next_state, winner, reward = env.take_action(action)\n",
    "\n",
    "    Q1[state][action] = Q1[state][action] + LEARNING_RATE * \\\n",
    "        (reward + GAMMA * np.max(Q1[next_state]) - Q1[state][action])\n",
    "\n",
    "    x_state, x_action = state, action\n",
    "\n",
    "    if winner and winner != \"Draw\":\n",
    "      rewards1.append(reward)\n",
    "      Q2[o_state][o_action] = Q2[o_state][o_action] + LEARNING_RATE * \\\n",
    "          (-1.0 + GAMMA * np.max(Q2[state]) - Q2[o_state][o_action])\n",
    "      rewards2.append(-1.0)\n",
    "      epsilon -= 0.001\n",
    "      break  # reached goal\n",
    "    elif winner == \"Draw\":\n",
    "      rewards1.append(reward)\n",
    "      Q2[o_state][o_action] = Q2[o_state][o_action] + LEARNING_RATE * \\\n",
    "          (0.5 + GAMMA * np.max(Q2[state]) - Q2[o_state][o_action])\n",
    "      rewards2.append(0.5)\n",
    "      epsilon -= 0.001\n",
    "      break  # reached goal\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "      action = env.get_action()\n",
    "    else:\n",
    "      action = np.argmax(Q2[state])\n",
    "\n",
    "    next_state, winner, reward = env.take_action(action)\n",
    "\n",
    "    Q2[state][action] = Q2[state][action] + LEARNING_RATE * \\\n",
    "        (reward + GAMMA * np.max(Q2[next_state]) - Q2[state][action])\n",
    "\n",
    "    o_state, o_action = state, action\n",
    "\n",
    "    if winner:\n",
    "      rewards2.append(reward)\n",
    "      Q1[x_state][x_action] = Q1[x_state][x_action] + LEARNING_RATE * \\\n",
    "          (-1.0 + GAMMA * np.max(Q1[state]) - Q1[x_state][x_action])\n",
    "      rewards1.append(-1.0)\n",
    "      epsilon -= 0.001\n",
    "      break  # reached goal\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "# print(Q1)\n",
    "print(f\"Average reward for X: {sum(rewards1)/len(rewards1)}: {rewards1[-10:-5]}\")\n",
    "print(f\"Average reward for O: {sum(rewards2)/len(rewards2)}: {rewards2[-10:-5]}\")\n",
    "# and now we can see our Q values!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Q\n",
    "fileName = \"q1.npy\"\n",
    "pathToFile = \"Q_Tables/\"+fileName\n",
    "data = dict(Q1)\n",
    "np.save(pathToFile, data)\n",
    "fileName = \"q2.npy\"\n",
    "pathToFile = \"Q_Tables/\"+fileName\n",
    "data = dict(Q2)\n",
    "np.save(pathToFile, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "204b4dafd421f8b1d768ea335da63d993a88ce91402f3aa6f9c08163cc9723cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
